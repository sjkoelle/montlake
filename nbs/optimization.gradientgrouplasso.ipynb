{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527e6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp optimization.gradientgrouplasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff3afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "#loosely inspired by the pyglmnet package\n",
    "from einops import rearrange\n",
    "#import autograd.numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class GradientGroupLasso:\n",
    "    \n",
    "    def __init__(self, dg_M, df_M, reg_l1s, reg_l2, max_iter,learning_rate, tol, beta0_npm= None):\n",
    "        \n",
    "        n = dg_M.shape[0]\n",
    "        d= dg_M.shape[1]\n",
    "        m = df_M.shape[2]\n",
    "        p = dg_M.shape[2]\n",
    "        dummy_beta = np.ones((n,p,m))\n",
    "        \n",
    "        self.dg_M = dg_M\n",
    "        self.df_M = df_M\n",
    "        self.reg_l1s = reg_l1s\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.beta0_npm = beta0_npm\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.m = m \n",
    "        self.d = d\n",
    "        self.dummy_beta = dummy_beta\n",
    "        #self.group = np.asarray(group)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "        \n",
    "    def _prox(self,beta_npm, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        p = self.p\n",
    "        result = np.zeros(beta_npm.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        for j in range(p):\n",
    "            if np.linalg.norm(beta_npm[:,j,:]) > 0.:\n",
    "                potentialoutput = beta_npm[:,j,:] - (thresh / np.linalg.norm(beta_npm[:,j,:])) * beta_npm[:,j,:]\n",
    "                posind = np.asarray(np.where(beta_npm[:,j,:] > 0.))\n",
    "                negind = np.asarray(np.where(beta_npm[:,j,:] < 0.))\n",
    "                po = beta_npm[:,j,:].copy()\n",
    "                po[posind[0],posind[1]] = np.asarray(np.clip(potentialoutput[posind[0],posind[1]],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind[0],negind[1]] = np.asarray(np.clip(potentialoutput[negind[0],negind[1]],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[:,j,:] = po\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta_npm):\n",
    "        \n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        reg_l2 = self.reg_l2\n",
    "        dummy_beta = self.dummy_beta\n",
    "        \n",
    "        df_M_hat = np.einsum('ndp,npm->ndm',dg_M, beta_npm)\n",
    "        error = df_M_hat - df_M\n",
    "        grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n",
    "        #if \n",
    "        return grad_beta\n",
    "    \n",
    "    def _L1penalty(self, beta_npm):\n",
    "        \n",
    "        p = self.p\n",
    "        m = self.m\n",
    "        n = self.n \n",
    "        beta_mn_p = rearrange(beta_npm, 'n p m -> (m n) p')#np.reshape(beta_mnp, ((m*n,p)))\n",
    "        L1penalty = np.linalg.norm(beta_mn_p, axis = 0).sum()\n",
    "        \n",
    "        return L1penalty\n",
    "    \n",
    "    def _loss(self,beta_npm, reg_lambda):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta_npm)\n",
    "        P = self._L1penalty(beta_npm)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL(self,beta_npm):\n",
    "        \n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        \n",
    "        df_M_hat = np.einsum('ndp,npm -> ndm',dg_M, beta_npm)\n",
    "        logL = -0.5 * np.linalg.norm((df_M - df_M_hat))**2\n",
    "        return(logL)\n",
    "    \n",
    "    def _L2loss(self,beta_npm):\n",
    "        output = -self._logL(beta_npm)\n",
    "        return(output)\n",
    "\n",
    "    def fhatlambda(self,learning_rate,beta_npm_new,beta_npm_old):\n",
    "\n",
    "        #print('lr',learning_rate)\n",
    "        output = self._L2loss(beta_npm_old) + np.einsum('npm,npm', self._grad_L2loss(beta_npm_old),(beta_npm_new-beta_npm_old)) + (1/(2*learning_rate)) * np.linalg.norm(beta_npm_new-beta_npm_old)**2\n",
    "        \n",
    "        return(output)\n",
    "\n",
    "    def _btalgorithm(self,beta_npm ,learning_rate,b,maxiter_bt,rl):\n",
    "        \n",
    "        grad_beta = self._grad_L2loss(beta_npm = beta_npm)\n",
    "        for i in range(maxiter_bt):\n",
    "            beta_npm_postgrad = beta_npm - learning_rate * grad_beta\n",
    "            beta_npm_postgrad_postprox = self._prox(beta_npm_postgrad, learning_rate * rl)\n",
    "            fz = self._L2loss(beta_npm_postgrad_postprox)\n",
    "            #fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm_postgrad)\n",
    "            fhatz = self.fhatlambda(learning_rate,beta_npm_postgrad_postprox, beta_npm)\n",
    "            if fz <= fhatz:\n",
    "                #print(i)\n",
    "                break\n",
    "            learning_rate = b*learning_rate    \n",
    "            \n",
    "        return(beta_npm_postgrad_postprox,learning_rate)\n",
    "    \n",
    "    def fit(self, beta0_npm = None):\n",
    "\n",
    "        reg_l1s = self.reg_l1s\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        p = self.p\n",
    "        \n",
    "        dg_M = self.dg_M\n",
    "        df_M = self.df_M\n",
    "        \n",
    "        tol = self.tol\n",
    "        np.random.RandomState(0)\n",
    "        \n",
    "        if beta0_npm is None:\n",
    "            beta_npm_hat = 1 / (n*m*p) * np.random.normal(0.0, 1.0, [n, p,m])\n",
    "            #1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        else: \n",
    "            beta_npm_hat = beta0_npm\n",
    "            \n",
    "        fit_params = list()\n",
    "        for l, rl in enumerate(reg_l1s):\n",
    "            fit_params.append({'beta': beta_npm_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_npm_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            \n",
    "            alpha = 1.\n",
    "            beta_npm_hat = fit_params[-1]['beta']\n",
    "            #g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            learning_rate = self.learning_rate\n",
    "            beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "            beta_npm_hat_2 = beta_npm_hat.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                #print(t,l,rl)\n",
    "                #print(t)\n",
    "                L.append(self._loss(beta_npm_hat, rl))\n",
    "                L2.append(self._L2loss(beta_npm_hat))\n",
    "                PEN.append(self._L1penalty(beta_npm_hat))\n",
    "                w = (t / (t+ 3))\n",
    "                beta_npm_hat_momentumguess = beta_npm_hat + w*(beta_npm_hat_1 - beta_npm_hat_2)\n",
    "                \n",
    "                beta_npm_hat , learning_rate = self._btalgorithm(beta_npm_hat_momentumguess,learning_rate,.5,1000, rl)\n",
    "                #print(beta_npm_hat_momentumguess.max(), beta_npm_hat.max(),self._L2loss(beta_npm_hat), learning_rate)\n",
    "                beta_npm_hat_2 = beta_npm_hat_1.copy()\n",
    "                beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "                \n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            fit_params[-1]['beta'] = beta_npm_hat\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        #self.ynull_ = np.mean(y)\n",
    "\n",
    "        return self\n",
    "\n",
    "def get_sr_lambda_parallel(df_M, dg_M, gl_itermax,reg_l2, max_search, card, tol,learning_rate):\n",
    "    \n",
    "    print('initializing lambda search')\n",
    "    highprobes = np.asarray([])\n",
    "    lowprobes = np.asarray([])\n",
    "    #lambdas_start = np.asarray([0,1])\n",
    "    ul = np.linalg.norm(np.einsum('n d m, n d p -> n p m ', df_M, dg_M),axis=tuple([0, 2])).max()\n",
    "    probe_init_low = 0.\n",
    "    probe_init_high = ul\n",
    "\n",
    "    coeffs = {}\n",
    "    combined_norms = {}\n",
    "\n",
    "    GGL = GradientGroupLasso(dg_M, df_M, np.asarray([probe_init_low]), reg_l2, gl_itermax,learning_rate, tol, beta0_npm= None)\n",
    "    GGL.fit()\n",
    "    beta0_npm = GGL.fit_[-1]['beta']\n",
    "    coeffs[probe_init_low] = GGL.fit_[-1]['beta']\n",
    "    combined_norms[probe_init_low] = np.sqrt((np.linalg.norm(coeffs[probe_init_low], axis = 0)**2).sum(axis = 1))\n",
    "\n",
    "    GGL = GradientGroupLasso(dg_M, df_M, np.asarray([probe_init_high]), reg_l2, gl_itermax,learning_rate, tol, beta0_npm= beta0_npm)\n",
    "    GGL.fit(beta0_npm = beta0_npm)\n",
    "    coeffs[probe_init_high] = GGL.fit_[-1]['beta']\n",
    "    combined_norms[probe_init_high] = np.sqrt((np.linalg.norm(coeffs[probe_init_high], axis = 0)**2).sum(axis = 1))\n",
    "\n",
    "    n_comp = len(np.where(~np.isclose(combined_norms[probe_init_high],0.,1e-12))[0])\n",
    "    lowprobes = np.append(lowprobes, probe_init_low)\n",
    "\n",
    "    if n_comp == card:\n",
    "        #high_int = probe\n",
    "        print('Selected functions',np.where(~np.isclose(combined_norms[probe_init_high],0.,1e-12))[0])\n",
    "        return (probe_init_high, coeffs, combined_norms)\n",
    "\n",
    "    if n_comp < card:\n",
    "        highprobes = np.append(highprobes, probe_init_high)\n",
    "        probe = (lowprobes.max() + highprobes.min()) / 2\n",
    "    if n_comp > card:\n",
    "        lowprobes = np.append(lowprobes, probe_init_high)\n",
    "        probe = lowprobes.max() * 2\n",
    "\n",
    "    for i in range(max_search):\n",
    "        print(i, probe, 'probe')\n",
    "        beta0_npm = coeffs[lowprobes.max()]\n",
    "        if not np.isin(probe, list(combined_norms.keys())):\n",
    "            #print('probe',probe)\n",
    "            GGL = GradientGroupLasso(dg_M, df_M, np.asarray([probe]), reg_l2, gl_itermax,learning_rate, tol, beta0_npm = beta0_npm)\n",
    "            GGL.fit(beta0_npm = beta0_npm)\n",
    "            coeffs[probe] = GGL.fit_[-1]['beta']\n",
    "            combined_norms[probe] = np.sqrt((np.linalg.norm(coeffs[probe], axis = 0)**2).sum(axis = 1))\n",
    "\n",
    "        #np.where(~np.isclose(np.linalg.norm(np.linalg.norm(GGL.fit_[-1]['beta'], axis=2), axis=0) ,0., 1e-16))[0]\n",
    "        #n_comp = len(np.where(combined_norms[probe] != 0)[0])\n",
    "        n_comp = len(np.where(~np.isclose(combined_norms[probe],0.,1e-12))[0])\n",
    "        if n_comp == card:\n",
    "            #high_int = probe\n",
    "            print('Selected functions',np.where(~np.isclose(combined_norms[probe],0.,1e-12))[0])\n",
    "            return(probe, coeffs, combined_norms)\n",
    "        else:\n",
    "            if n_comp < card:\n",
    "                highprobes = np.append(highprobes, probe)\n",
    "            if n_comp > card:\n",
    "                lowprobes = np.append(lowprobes, probe)\n",
    "            if len(highprobes) > 0:\n",
    "                probe = (lowprobes.max() + highprobes.min()) / 2\n",
    "            else:\n",
    "                probe = lowprobes.max() * 2\n",
    "            if i == max_search - 1:\n",
    "                print('Failed to select d functions')\n",
    "                return(probe, coeffs, combined_norms)\n",
    "            \n",
    "def batch_stream(replicates):\n",
    "    \n",
    "    reps = np.asarray(list(replicates.keys()))\n",
    "    nreps= len(reps)\n",
    "    for r in range(nreps):\n",
    "        yield(replicates[r])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b75bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_env_april",
   "language": "python",
   "name": "manifold_env_april"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
